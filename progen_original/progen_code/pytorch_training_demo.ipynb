{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4bf2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 15:13:02.537862: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-15 15:13:11.051387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import pdb\n",
    "import numpy as np\n",
    "import platform\n",
    "import hashlib\n",
    "import pytorch_transformer\n",
    "import re\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformProtein import transformProtein\n",
    "from ProteinDataset import ProteinDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757d6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_py3 = platform.python_version()[0] == '3'\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch code for generating from CTRL')\n",
    "\n",
    "#parser.add_argument('--model_dir', type =str, default='model_v0.pth', help='location of training model checkpoint')\n",
    "#parser.add_argument('--model_path', type=str, default='/home/amadani/ctrl/ckpt/seqlen256_36layers_v0.ckpt/model.ckpt-684000', help='location of model *data* checkpoint to load; this is NOT the directory but rather the model checkpoint')\n",
    "\n",
    "parser.add_argument('--model_dir', type =str, default='./checkpoints_cur/finetune_progen_full_demo.pth', help='location of training model checkpoint')\n",
    "parser.add_argument('--model_path', type=str, default='../checkpoints/pretrain_progen_full.pth', help='location of model *data* checkpoint to load; this is NOT the directory but rather the model checkpoint')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=313,\n",
    "                                        help='random seed for PyTorch, numpy and PythonHash')\n",
    "parser.add_argument('--sequence_len', type=int, default=511,\n",
    "                                        help='sequence len of model being fine-tuned')\n",
    "parser.add_argument('--num_epochs', type=int, default=15, help='number of epochs to train for')\n",
    "parser.add_argument('--num_layers', type=int, default=36, help='number of transfomer layers. used for loading checkpoint')\n",
    "parser.add_argument('--batch_size', type=int, default = 4, help='batch size for dataloader')\n",
    "parser.add_argument('--vocab_loc', type=str, default='mapping_files/vocab.txt', help='vocab location')\n",
    "parser.add_argument('--num_workers', type=int, default=0, help='for dataloader')\n",
    "parser.add_argument('--warmup_iteration', type=int, default=1000, help='LR warmup cutoff')\n",
    "parser.add_argument('--save_iter', type=int, default=1000, help='save model checkpoint every X iterations')\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c02f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----vocab size 129407 ------\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary from file\n",
    "vocab = open(args.vocab_loc).readlines() if not use_py3 else open(args.vocab_loc, encoding='utf-8').read().split('\\n')[:-1]\n",
    "vocab = list(map(lambda x: x.split(' ')[0], vocab))\n",
    "# length of the vocabulary\n",
    "vocab_size = len(vocab)\n",
    "print('-----vocab size',vocab_size,'------')\n",
    "\n",
    "# define the numericalization map\n",
    "# idx2word maps the numericalized ID to the word\n",
    "# word2idx maps the word to the numericalized ID\n",
    "#word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "#idx2word = np.array(vocab)\n",
    "\n",
    "# sequence length to use for transfomer\n",
    "seq_length = args.sequence_len\n",
    "embedding_dim = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1695df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiedEmbeddingSoftmax(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size=vocab_size, embedding_size=embedding_dim, **kwargs):\n",
    "    super(TiedEmbeddingSoftmax, self).__init__()\n",
    "    self.w = torch.nn.Parameter(torch.normal(0., 1e-2, size=(vocab_size, embedding_size)))\n",
    "    self.b = torch.nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "  def forward(self, inputs, embed=True):\n",
    "    if embed:\n",
    "      return torch.nn.functional.embedding(inputs, self.w)\n",
    "    else:\n",
    "      return torch.tensordot(inputs, self.w.t(), 1) + self.b\n",
    "\n",
    "class CTRLmodel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CTRLmodel,self).__init__()\n",
    "    self.tied_embedding_softmax = TiedEmbeddingSoftmax()\n",
    "    self.encoder = pytorch_transformer.Encoder()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    x = self.tied_embedding_softmax(inputs, embed = True)\n",
    "    x = self.encoder(x)\n",
    "    x = self.tied_embedding_softmax(x, embed = False)\n",
    "    return x\n",
    "\n",
    "  def loadCheckpoint(self, model_path, num_layers):\n",
    "    #pytorch_model_hash = hashlib.md5(model_path.encode('utf-8')).hexdigest()\n",
    "    pytorch_model_hash = model_path\n",
    "\n",
    "    if os.path.exists(pytorch_model_hash):\n",
    "      print('Found PyTorch checkpoint @', pytorch_model_hash)\n",
    "      print('Loading instead of converting from TensorFlow')\n",
    "      checkpoint = torch.load(pytorch_model_hash)\n",
    "      \n",
    "      #self.tied_embedding_softmax.load_state_dict(checkpoint['softmax'])\n",
    "      #self.encoder.load_state_dict(checkpoint['encoder'])\n",
    "      ## load state dict has KeyError, because checkpoint is ready the state_dict\n",
    "      ## can load checkpoint directly \n",
    "      ## https://discuss.pytorch.org/t/keyerror-state-dict/18220/5\n",
    "      self.load_state_dict(checkpoint)\n",
    "\n",
    "      self.tied_embedding_softmax.to('cuda')\n",
    "      self.encoder.to('cuda')\n",
    "\n",
    "    else:\n",
    "      print('Error: Could not find PyTorch checkpoint')\n",
    "      sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ae68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "# initialize ctrl object\n",
    "model = CTRLmodel()\n",
    "print('model initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9564ae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PyTorch checkpoint @ ../checkpoints/pretrain_progen_full.pth\n",
      "Loading instead of converting from TensorFlow\n",
      "previous checkpoint loaded\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint with args.model_path\n",
    "model.loadCheckpoint(model_path=args.model_path, num_layers = args.num_layers)\n",
    "print('previous checkpoint loaded')\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8762359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all weights except embedding\n",
    "for p in model.parameters():\n",
    "    p.requires_grad=False\n",
    "model.tied_embedding_softmax.w.requires_grad=True\n",
    "model.tied_embedding_softmax.b.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b49777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, warmup_iteration, seq_length, batch_size, num_workers, vocab_size, model_dir, save_iter):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_dir = model_dir\n",
    "        self.save_iter = save_iter\n",
    "        self.firstAAidx = self.vocab_size - 26 # Assuming that the pad token is the last token and AAs are at the end\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters()) #lr, betas\n",
    "        lambdafn = lambda iteration: min(iteration/(warmup_iteration*1.0),1.0)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambdafn)\n",
    "        \n",
    "        self.criterion = torch.nn.CrossEntropyLoss(ignore_index=self.vocab_size-1, reduction='none')\n",
    "        \n",
    "        self.transformFull = transformProtein(maxSampleLength = seq_length+1, \n",
    "                                              selectSwiss = 1.0, selectTrembl = 0, \n",
    "                                              maxTaxaPerSample = 3, maxKwPerSample = 5, dropRate = 0.2)\n",
    "        self.transformPartial = transformProtein(maxSampleLength = seq_length+1,   \n",
    "                                                 selectSwiss = 1.0, selectTrembl = 0,\n",
    "                                                 maxTaxaPerSample = 3, maxKwPerSample = 5, dropRate = 0.2)\n",
    "        self.transformNone = transformProtein(maxSampleLength = seq_length+1,   \n",
    "                                              selectSwiss = 1.0, selectTrembl = 0,\n",
    "                                              maxTaxaPerSample = 3, maxKwPerSample = 5, dropRate = 0.2)\n",
    "        \n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        self.model.train()\n",
    "\n",
    "        iter_num = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            loss_e = 0.0\n",
    "            num_e = 0\n",
    "\n",
    "            for chunknum in range(1):\n",
    "                pklpath = '../miBIG/mibig_train_new.p'\n",
    "                chunk_dataset = ProteinDataset(pklpath, firstAAidx = self.firstAAidx, transformFull = self.transformFull, \n",
    "                                               transformPartial = self.transformPartial, transformNone = self.transformNone)\n",
    "                dataloader = DataLoader(chunk_dataset, shuffle = True, batch_size = self.batch_size,\n",
    "                                        num_workers = self.num_workers, pin_memory = False) #TODO pinmem?\n",
    "                \n",
    "                for i, (sample, labels, existence, padIndex, begAAindex) in enumerate(dataloader):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    sample, labels, existence, padIndex = sample.cuda(), labels.cuda(), existence.cuda(), padIndex.cuda()\n",
    "                    output = self.model(sample)\n",
    "                    #pdb.set_trace()\n",
    "                    loss = self.criterion(output.permute(0,2,1), labels)\n",
    "                    loss = torch.mean((torch.sum(loss,dim=1)/padIndex)*existence) #pad masking, loss weighting\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    loss_e += loss.item()\n",
    "                    num_e += sample.shape[0]\n",
    "                    iter_num += 1\n",
    "                    self.writer.add_scalar('Loss_iteration',loss.item(),iter_num)\n",
    "\n",
    "                    if (iter_num+1)%self.save_iter==0:\n",
    "                        torch.save({'epoch': epoch, 'chunknum': chunknum, 'iteration':iter_num,\n",
    "                                    'model_state_dict': self.model.state_dict(),\n",
    "                                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                                    'loss': loss,\n",
    "                                   }, self.model_dir)\n",
    "                loss_e/=num_e\n",
    "            print(\"Epoch: {0} ; loss_e: {1}\".format(epoch, loss_e))\n",
    "            self.writer.add_scalar('Loss_epoch',loss_e, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe459ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using one unified encoder to represent protein sample with length 129406\n",
      "loaded UID: BGC0001135\n",
      "loaded UID: BGC0001348\n",
      "loaded UID: BGC0002135\n",
      "loaded UID: BGC0001185\n",
      "loaded UID: BGC0000431\n",
      "loaded UID: BGC0000114\n",
      "loaded UID: BGC0001873\n",
      "loaded UID: BGC0001133\n",
      "loaded UID: BGC0002092\n",
      "loaded UID: BGC0001462\n",
      "loaded UID: BGC0001164\n",
      "loaded UID: BGC0000175\n",
      "loaded UID: BGC0001875\n",
      "loaded UID: BGC0000302\n",
      "loaded UID: BGC0002086\n",
      "loaded UID: BGC0001437\n",
      "loaded UID: BGC0000290\n",
      "loaded UID: BGC0000386\n",
      "loaded UID: BGC0001163\n",
      "loaded UID: BGC0001838\n",
      "loaded UID: BGC0000171\n",
      "loaded UID: BGC0002078\n",
      "loaded UID: BGC0002071\n",
      "loaded UID: BGC0001338\n",
      "loaded UID: BGC0000427\n",
      "loaded UID: BGC0002091\n",
      "loaded UID: BGC0000208\n",
      "loaded UID: BGC0000399\n",
      "loaded UID: BGC0001844\n",
      "loaded UID: BGC0000306\n",
      "loaded UID: BGC0001459\n",
      "loaded UID: BGC0001460\n",
      "loaded UID: BGC0001283\n",
      "loaded UID: BGC0000446\n",
      "loaded UID: BGC0001118\n",
      "loaded UID: BGC0002051\n",
      "loaded UID: BGC0000165\n",
      "loaded UID: BGC0000357\n",
      "loaded UID: BGC0002137\n",
      "loaded UID: BGC0002081\n",
      "loaded UID: BGC0001831\n",
      "loaded UID: BGC0000442\n",
      "loaded UID: BGC0001160\n",
      "loaded UID: BGC0001446\n",
      "loaded UID: BGC0001123\n",
      "loaded UID: BGC0001461\n",
      "loaded UID: BGC0001132\n",
      "loaded UID: BGC0000416\n",
      "loaded UID: BGC0001872\n",
      "loaded UID: BGC0001629\n",
      "loaded UID: BGC0000818\n",
      "loaded UID: BGC0000384\n",
      "loaded UID: BGC0001442\n",
      "loaded UID: BGC0001833\n",
      "loaded UID: BGC0000319\n",
      "loaded UID: BGC0002079\n",
      "loaded UID: BGC0000333\n",
      "loaded UID: BGC0001836\n",
      "loaded UID: BGC0001877\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Check the format of training data before the model train\n",
    "##########################################################\n",
    "pklpath = '../miBIG/mibig_train_new.p'\n",
    "obj = transformProtein(mapfold = \"./mapping_files\", selectSwiss = 1.0, selectTrembl = 0, maxTaxaPerSample = 3, maxKwPerSample = 5, dropRate = 0.2)\n",
    "with open(pklpath, 'rb') as handle:\n",
    "    train_chunk = pickle.load(handle)\n",
    "for uid in train_chunk.keys():\n",
    "  try:\n",
    "    sample_arr, existence, thePadIndex = obj.transformSample(train_chunk[uid])\n",
    "    print(\"loaded UID:\", uid)\n",
    "  except:\n",
    "    print(\"Error UID:\", uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cee3398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using one unified encoder to represent protein sample with length 129406\n",
      "Using one unified encoder to represent protein sample with length 129406\n",
      "Using one unified encoder to represent protein sample with length 129406\n",
      "begin training...\n",
      "Epoch: 0 ; loss_e: 1.407701136702198\n",
      "Epoch: 1 ; loss_e: 1.4022442122637215\n",
      "Epoch: 2 ; loss_e: 1.3921550977028023\n",
      "Epoch: 3 ; loss_e: 1.4082606364104708\n",
      "Epoch: 4 ; loss_e: 1.3845259294671528\n",
      "Epoch: 5 ; loss_e: 1.3901815495248568\n",
      "Epoch: 6 ; loss_e: 1.3784440897278867\n",
      "Epoch: 7 ; loss_e: 1.3864566027107885\n",
      "Epoch: 8 ; loss_e: 1.364256333496611\n",
      "Epoch: 9 ; loss_e: 1.364667520684711\n",
      "Epoch: 10 ; loss_e: 1.3570044323549433\n",
      "Epoch: 11 ; loss_e: 1.3604386943881794\n",
      "Epoch: 12 ; loss_e: 1.3498359615519895\n",
      "Epoch: 13 ; loss_e: 1.3464062820046634\n",
      "Epoch: 14 ; loss_e: 1.338046413356975\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Train the model\n",
    "##########################################################\n",
    "training = Trainer(model=model, warmup_iteration=args.warmup_iteration, seq_length=seq_length,\n",
    "                   batch_size=args.batch_size, num_workers=args.num_workers, vocab_size=vocab_size,\n",
    "                   model_dir = args.model_dir, save_iter=args.save_iter)\n",
    "print('begin training...')\n",
    "training.train(args.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ddf911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using one unified encoder to represent protein sample with length 129406\n",
      "loaded UID: BGC0001135\n",
      "loaded UID: BGC0001348\n",
      "loaded UID: BGC0002135\n",
      "loaded UID: BGC0001185\n",
      "loaded UID: BGC0000431\n",
      "loaded UID: BGC0000114\n",
      "loaded UID: BGC0001873\n",
      "loaded UID: BGC0001133\n",
      "loaded UID: BGC0002092\n",
      "loaded UID: BGC0001462\n",
      "loaded UID: BGC0001164\n",
      "loaded UID: BGC0000175\n",
      "loaded UID: BGC0001875\n",
      "loaded UID: BGC0000302\n",
      "loaded UID: BGC0002086\n",
      "loaded UID: BGC0001437\n",
      "loaded UID: BGC0000290\n",
      "loaded UID: BGC0000386\n",
      "loaded UID: BGC0001163\n",
      "loaded UID: BGC0001838\n",
      "loaded UID: BGC0000171\n",
      "loaded UID: BGC0002078\n",
      "loaded UID: BGC0002071\n",
      "loaded UID: BGC0001338\n",
      "loaded UID: BGC0000427\n",
      "loaded UID: BGC0002091\n",
      "loaded UID: BGC0000208\n",
      "loaded UID: BGC0000399\n",
      "loaded UID: BGC0001844\n",
      "loaded UID: BGC0000306\n",
      "loaded UID: BGC0001459\n",
      "loaded UID: BGC0001460\n",
      "loaded UID: BGC0001283\n",
      "loaded UID: BGC0000446\n",
      "loaded UID: BGC0001118\n",
      "loaded UID: BGC0002051\n",
      "loaded UID: BGC0000165\n",
      "loaded UID: BGC0000357\n",
      "loaded UID: BGC0002137\n",
      "loaded UID: BGC0002081\n",
      "loaded UID: BGC0001831\n",
      "loaded UID: BGC0000442\n",
      "loaded UID: BGC0001160\n",
      "loaded UID: BGC0001446\n",
      "loaded UID: BGC0001123\n",
      "loaded UID: BGC0001461\n",
      "loaded UID: BGC0001132\n",
      "Error UID: BGC0000274\n",
      "loaded UID: BGC0000416\n",
      "loaded UID: BGC0001872\n",
      "loaded UID: BGC0001629\n",
      "loaded UID: BGC0000818\n",
      "loaded UID: BGC0000384\n",
      "loaded UID: BGC0001442\n",
      "loaded UID: BGC0001833\n",
      "loaded UID: BGC0000319\n",
      "loaded UID: BGC0002079\n",
      "loaded UID: BGC0000333\n",
      "loaded UID: BGC0001836\n",
      "loaded UID: BGC0001877\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Check the format of training data before the model train\n",
    "##########################################################\n",
    "pklpath = '../miBIG/mibig_train.p'\n",
    "obj = transformProtein(mapfold = \"./mapping_files\", selectSwiss = 1.0, selectTrembl = 0, maxTaxaPerSample = 3, maxKwPerSample = 5, dropRate = 0.2)\n",
    "with open(pklpath, 'rb') as handle:\n",
    "    train_chunk = pickle.load(handle)\n",
    "for uid in train_chunk.keys():\n",
    "  try:\n",
    "    sample_arr, existence, thePadIndex = obj.transformSample(train_chunk[uid])\n",
    "    print(\"loaded UID:\", uid)\n",
    "  except:\n",
    "    print(\"Error UID:\", uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557274aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b5902f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'swiss': {'BGC0000274': {'ex': 1, 'kw': ['Polyketide'], 'taxa': [77133]}},\n",
       " 'seq': 'TENVLTLLRRPSDGLGVADEAISVRYARRLHGTIELLALSDYGLSAFDRDLASEDLAGDESEGACTRLVELLEDTSM WSRSEIYSRMAKQNAETHEAMREAWANEEHKREAPLEFYMDWTMHTRNGPMAHYVWTLQNFVFPAVPESREAYALYPPYLMRWSIWSLNDQNTLRFWIKRGERRLIETELYEPVIRPWQDIRNSVDFTVGVPADVMVSRTVVPM SPVLAVARTWAATAGDILVPFAIVLFGQEPLAALNHLRELQYYPRRRGLMHAPWLVDRDGTALYEEIMPPFPGDFSAADTGIVTIGADIFAEVVDAGIAKAGGTHILGIAGAPGVGDAVRAAPDVHRLGIVREPVDRVDWRFGQGIFSRLPAENVRGGAGAPGYHGPADVHSGAHVSARVMEHRLFAGDPFDEPRFRGVAEPVLEPLVHQCFFAAGPGPELVELETRFPGDAHTSVDSSLDVIRCAAIGAVLERMGTVTKAEWEGRGDFGGHRM'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk['BGC0000274']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aca575a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_arr, existence, thePadIndex \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformSample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_chunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBGC0000274\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/scratch/users/fengchenliu/GitHub/progen_original/progen_code/transformProtein.py:139\u001b[0m, in \u001b[0;36mtransformProtein.transformSample\u001b[0;34m(self, proteinDict, justidx)\u001b[0m\n\u001b[1;32m    137\u001b[0m seq_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(encodedSample)\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxSampleLength) \u001b[38;5;129;01mand\u001b[39;00m (seq_idx\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mlen\u001b[39m(seq)):\n\u001b[0;32m--> 139\u001b[0m     encodedSample\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maa_to_ctrl\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    140\u001b[0m     seq_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    141\u001b[0m thePadIndex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(encodedSample)\n",
      "\u001b[0;31mKeyError\u001b[0m: ' '"
     ]
    }
   ],
   "source": [
    "sample_arr, existence, thePadIndex = obj.transformSample(train_chunk['BGC0000274'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18907b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "progen-salesforce",
   "language": "python",
   "name": "progen-salesforce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
